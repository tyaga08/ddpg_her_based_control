{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.thate*(self.mu - self.x_prev)*self.dt + self.sigma*np.sqrt(self.dt)*np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HERbuffer(object):\n",
    "    def __init__(self, inp_shape, num_actions):\n",
    "        self.inp_shape = inp_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.state_mem = np.zeros(inp_shape)\n",
    "        self.new_state_mem = np.zeros(inp_shape)\n",
    "        self.action_mem = np.zeros(num_actions)\n",
    "        self.reward_mem = 0\n",
    "        self.terminal_mem = False\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done=False):\n",
    "        self.state_mem = state\n",
    "        self.action_mem = action\n",
    "        self.new_state_mem = new_state\n",
    "        self.reward_mem = reward\n",
    "        self.terminal_mem = 1 - int(done)\n",
    "\n",
    "    #def manipulate_buffer(self):\n",
    "        #r,c = self.new_state_mem.shape\n",
    "        #desired_goal = new_state_mem[-1][]\n",
    "\n",
    "\n",
    "    def get_her_buffer(self, state, action, reward, new_state, done=False):\n",
    "        store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "        return self.state_mem, self.action, self.reward, self.new_state, self.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchSlide-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 9.95644936e-01,  7.48909349e-01,  4.12685879e-01,  9.21860028e-01,\n",
       "         6.63677386e-01,  4.14022562e-01, -7.37849079e-02, -8.52319625e-02,\n",
       "         1.33668285e-03, -2.02409822e-06,  1.46269158e-03, -5.32964268e-03,\n",
       "         1.24548653e-04, -1.88202233e-02,  1.18415598e-03, -5.54487049e-05,\n",
       "         7.46713768e-05,  1.86450993e-02, -4.64845349e-04, -4.83036132e-03,\n",
       "        -1.22686993e-03,  5.01588816e-05, -2.30984296e-06,  4.63429471e-07,\n",
       "         5.47231104e-05]),\n",
       " 'achieved_goal': array([0.92186003, 0.66367739, 0.41402256]),\n",
       " 'desired_goal': array([1.38655201, 0.88080161, 0.41401894])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = HERbuffer(25,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = env.reset()\n",
    "    a = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((5,5))\n",
    "a[-1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-69662880372a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/vitual-envs/mujoco-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-b84459248698>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vitual-envs/mujoco-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vitual-envs/mujoco-env/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vitual-envs/mujoco-env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \"\"\"\n\u001b[0;32m-> 1368\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "actor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, inp_shape, num_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.inp_shape = inp_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.mem_cntr = 0\n",
    "        self.state_mem = np.zeros((self.mem_size, *inp_shape))\n",
    "        self.new_state_mem = np.zeros((self.mem_size, *inp_shape))\n",
    "        self.action_mem = np.zeros((self.mem_size, self.num_actions))\n",
    "        self.reward_mem = np.zeros(self.mem_size)\n",
    "        self.terminal_mem = np.zeros(self.mem_size, dtype=float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done=False):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_mem[index] = state\n",
    "        self.action_mem[index] = action\n",
    "        self.new_state_mem[index] = new_state\n",
    "        self.reward_mem[index] = reward\n",
    "        self.terminal_mem = 1 - int(done)\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        states = self.state_mem[batch]\n",
    "        new_states = self.new_state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        terminals = self.terminal_mem[batch]\n",
    "\n",
    "        return states, actions, new_states, rewards, terminals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNw(nn.Module):\n",
    "    def __init__(self, name, alpha, inp_dims, fc1_dims, fc2_dims, num_actions, action_bound=1, batch_size=64, chkp_dir='tmp/ddpg'):\n",
    "        super(ActorNw, self).__init__()\n",
    "        self.lr = alpha\n",
    "        self.name = name\n",
    "        self.inp_dims = inp_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.num_actions = num_actions\n",
    "        self.action_bound = action_bound\n",
    "        self.batch_size = batch_size\n",
    "        self.chkp_file = os.path.join(chkp_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = nn.Linear(self.inp_dims, self.fc1_dims)\n",
    "        f1_bound = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc1.weight.data, -f1_bound, f1_bound)\n",
    "        nn.init.uniform_(self.fc1.bias.data, -f1_bound, f1_bound)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2_bound = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc2.weight.data, -f2_bound, f2_bound)\n",
    "        nn.init.uniform_(self.fc2.bias.data, -f2_bound, f2_bound)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.num_actions)\n",
    "        f3_bound = 0.003\n",
    "        nn.init.uniform_(self.mu.weight.data, -f3_bound, f3_bound)\n",
    "        nn.init.uniform_(self.mu.bias.data, -f3_bound, f3_bound)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "\n",
    "        x = self.mu(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def save_checkpoint_to_file(self):\n",
    "        print('############  Saving checkpoint  ##############')\n",
    "        torch.save(self.state_dict(), self.chkp_file)\n",
    "\n",
    "    def load_checkpoint_from_file(self):\n",
    "        print('#############  Loading checkpoint  ###############')\n",
    "        self.load_state_dict(torch.load(self.chkp_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNw(nn.Module):\n",
    "    def __init__(self, name, beta, inp_dims, fc1_dims, fc2_dims, num_actions, action_bound=1, batch_size=64, chkp_dir='tmp/ddpg'):\n",
    "        super(ActorNw, self).__init__()\n",
    "        self.lr = beta\n",
    "        self.name = name\n",
    "        self.inp_dims = inp_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.num_actions = num_actions\n",
    "        self.action_bound = action_bound\n",
    "        self.batch_size = batch_size\n",
    "        self.chkp_file = os.path,join(chkp_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.inp_dims, self.fc1_dims)\n",
    "        f1_bound = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc1.weight.data, -f1_bound, f1_bound)\n",
    "        nn.init.uniform_(self.fc1.bias.data, -f1_bound, f1_bound)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2_bound = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc2.weight.data, -f2_bound, f2_bound)\n",
    "        nn.init.uniform_(self.fc2.bias.data, -f2_bound, f2_bound)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.num_actions, fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        f3_bound = 0.003\n",
    "        nn.init.uniform_(self.q.weight.data, -f3_bound, f3_bound)\n",
    "        nn.init.unifor_(self.q.bias.data, -f3_bound, f3_bound)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = F.relu(self.bn1(state_value))\n",
    "        \n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = F.relu(self.bn2(state_value))\n",
    "\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(torch.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint_to_file(self):\n",
    "        print('############  Saving checkpoint  ##############')\n",
    "        torch.save(self.state_dict(), self.chkp_file)\n",
    "\n",
    "    def load_checkpoint_from_file(self):\n",
    "        print('#############  Loading checkpoint  ###############')\n",
    "        self.load_state_dict(torch.load(self.chkp_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, tau, env, num_actions, layer1_size, layer2_size, gamma=0.99, batch_size=64, max_size=100000):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(max_size, input_dims,num_actions)\n",
    "\n",
    "        self.actor = ActorNw('Actor', alpha, input_dims, layer1_size, layer2_size, num_actions)\n",
    "\n",
    "        self.critic = CriticNw('Critic', beta, input_dims, layer1_size, layer2_size, num_actions)\n",
    "\n",
    "        self.target_actor = ActorNw('TargetActor', alpha, input_dims, layer1_size, layer2_size, num_actions)\n",
    "\n",
    "        self.target_critic = CriticNw('TargetCritic', beta, input_dims, layer1_size, layer2_size, num_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(num_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = torch.tensor(observation,dtype=torch.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + torch.tensor(self.noise(),dtype=torch.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detatch().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, new_states, rewards, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float).to(self.critic.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float).to(self.critic.device)\n",
    "        new_states = torch.tensor(new_states, dtype=torch.float).to(self.critic.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(self.critic.device)\n",
    "        done = torch.tensor(done).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        # Q_phi(s,a)\n",
    "        critic_value = self.critic.forward(states, actions)\n",
    "\n",
    "        # mu_theta_targ(s')\n",
    "        target_actions = self.target_actor.forward(new_states)\n",
    "\n",
    "        # Q_phi_targ(s', mu_theta_targ(s'))\n",
    "        critic_value_ = self.target_critic(new_state, target_actions)\n",
    "\n",
    "        # Find the target value for the batch size using the Bellman equation  \n",
    "        target_value = []\n",
    "        for idx in range(self.batch_size):\n",
    "            target_value.append(rewards[idx] + self.gamma*done[idx]*critic_value_[idx])\n",
    "\n",
    "        target_value = torch.tensor(target_value, dtype=float).to(self.critic.device)\n",
    "        target_value = target_value(self.batch_size,1)\n",
    "\n",
    "        # Gradient descent to minimize the loss function\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target_value, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "        self.critic.eval()\n",
    "        \n",
    "        # gradient ascent to find the optimal policy\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu_theta = self.actor.forward(states)\n",
    "        self.actor.train()\n",
    "        actor_obj_function = -self.critic.forward(states, mu_theta)\n",
    "        actor_obj_function = torch.mean(actor_obj_function)\n",
    "        actor_obj_function.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "\n",
    "        # Update step phi_targ = tau*phi_targ + (1 - tau)*phi \n",
    "        for key in critic_state_dict:\n",
    "            critic_state_dict[key] = tau*critic_state_dict[key].clone() + (1-tau)*target_critic_state_dict[key].clone()\n",
    "        self.target_critic.load_state_dict(actor_state_dict)\n",
    "\n",
    "        # Update step theta_targ = tau*theta_targ + (1 - tau)*theta \n",
    "        for key in actor_state_dict:\n",
    "            actor_state_dict[key] = tau*actor_state_dict[key].clone() + (1-tau)*target_actor_state_dict[key].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint_to_file()\n",
    "        self.critic.save_checkpoint_to_file()\n",
    "        self.target_actor.save_checkpoint_to_file()\n",
    "        self.target_critic.save_checkpoint_to_file()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint_from_file()\n",
    "        self.critic.load_checkpoint_from_file()\n",
    "        self.target_actor.load_checkpoint_from_file()\n",
    "        self.target_critic.load_checkpoint_from_file()\n",
    "\n",
    "    def check_actor_params(self):\n",
    "        current_actor_params = self.actor.named_parameters()\n",
    "        current_actor_dict = dict(current_actor_params)\n",
    "        original_actor_dict = dict(self.original_actor.named_parameters())\n",
    "        original_critic_dict = dict(self.original_critic.named_parameters())\n",
    "        current_critic_params = self.critic.named_parameters()\n",
    "        current_critic_dict = dict(current_critic_params)\n",
    "        print('Checking Actor parameters')\n",
    "\n",
    "        for param in current_actor_dict:\n",
    "            print(param, T.equal(original_actor_dict[param], current_actor_dict[param]))\n",
    "        print('Checking critic parameters')\n",
    "        for param in current_critic_dict:\n",
    "            print(param, T.equal(original_critic_dict[param], current_critic_dict[param]))\n",
    "        input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
